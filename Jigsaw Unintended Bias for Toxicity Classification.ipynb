{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from tqdm import tqdm \nimport re\nimport gc\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nimport gc\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences, sequence\nfrom keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate, Lambda, RepeatVector, Permute, multiply\nfrom keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\nfrom keras.optimizers import Adam, SGD\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.layers import concatenate\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\n\n#import os\n#print(os.listdir(\"../input/fasttext-crawl-300d-2m\"))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Constants"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"CRAWL_EMBEDDING_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nGLOVE_EMBEDDING_PATH = '../input/glove840b300dtxt/glove.840B.300d.txt'\nEMBED_SIZE = 600\nMAX_FEATURES = 100000\nMAX_LEN = 256","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(data):\n    '''\n    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n    '''\n    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n    def clean_special_chars(text, punct):\n        for p in punct:\n            text = text.replace(p, ' ')\n        return text\n\n    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocessing the training data and testing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n\nx_train = preprocess(train['comment_text'])\ny_train = np.where(train['target'] >= 0.5, 1, 0)\nx_test = preprocess(test['comment_text'])\nprint(x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tokenize the testing and training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=MAX_FEATURES)\ntokenizer.fit_on_texts(list(x_train))\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\n\n#Pad the sequences\nx_train = pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = pad_sequences(x_test, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logic to build the embedding matrix\n\nSource: https://www.kaggle.com/bminixhofer/simple-lstm-pytorch-version\n\nModifications: Does not keep track of the unknown words"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    \n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            continue\n    return embedding_matrix\n\nglove_matrix = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\ncrawl_matrix = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\n\nembedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n\ndel glove_matrix\ndel crawl_matrix\ngc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LSTM and MLP Definition\n\nAttention code source: https://github.com/keras-team/keras/issues/4962"},{"metadata":{"trusted":true},"cell_type":"code","source":"units = 64\ndef LSTM(embedding_matrix):\n    inp = Input(shape=(MAX_LEN,))\n    x = Embedding(embedding_matrix.shape[0], EMBED_SIZE, weights=[embedding_matrix], trainable=False)(inp)\n    \n    #Spatial Dropout layer\n    x = SpatialDropout1D(0.3)(x)\n    \n    #LSTM Layer\n    x = CuDNNLSTM(units, return_sequences=True)(x)\n    \n    \n    #attention layer\n    attention = Dense(1, activation='tanh')(x)\n    attention = Flatten()(attention)\n    attention = Activation('softmax')(attention)\n    attention = RepeatVector(units)(attention)\n    attention = Permute([2,1])(attention)\n    \n    merged = multiply([x, attention])\n    #Pooling Layers\n    average_pool = GlobalAveragePooling1D()(merged)\n    max_pool = GlobalMaxPooling1D()(merged)\n    \n    #combine all the outputs to input into MLP\n    concat_avg_max = concatenate([average_pool, max_pool])\n    \n    #MLP Definition\n    concat_avg_max = Dense(256, activation='relu')(concat_avg_max)\n    concat_avg_max = Dense(128, activation='relu')(concat_avg_max)\n    concat_avg_max = Dense(16, activation='relu')(concat_avg_max)\n    output = Dense(1, activation='sigmoid')(concat_avg_max)\n    \n    #Compiling the models together\n    model=Model(inputs=inp, outputs=output)\n    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-3, decay=0), metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Validation using Kfold"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_folds = 3\nnum_epochs = 4\nfolds = KFold(n_splits=num_folds, shuffle=True, random_state=53)\nmodel = LSTM(embedding_matrix)\ndef train(x_train, y_train, x_test):\n    prediction = np.zeros((len(x_test), 1))\n    \n    #KFold Validation\n    for fold_index, (train_index, valid_index) in enumerate(folds.split(x_train, y_train)):\n        x_train_split = x_train[train_index]\n        y_train_split = y_train[train_index]\n        x_validation = x_train[valid_index]\n        y_validation = y_train[valid_index]\n        \n        model.fit(x_train_split, y_train_split, batch_size = 512, epochs = num_epochs, validation_data = (x_validation, y_validation))\n        \n        prediction += model.predict(x_test, batch_size = 512, verbose = 1)\n        \n    prediction /= fold_index\n    \n    return prediction\nprediction = train(x_train, y_train, x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\nsubmission['prediction'] = prediction\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}